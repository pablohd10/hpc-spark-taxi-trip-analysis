{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408a82a5-1062-4d5f-b494-dea6caddf6fe",
   "metadata": {},
   "source": [
    "![LogoUC3M](data:image/png;base64,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)\n",
    "\n",
    "# PRÁCTICA 1: Procesamiento de datos mediante Apache Spark\n",
    "\n",
    "Autores: Pablo Hidalgo Delgado, Till Niklas Kobele, Juan Romero Sanz, Juan María Villard Bardón.\n",
    "\n",
    "NIAs: 100451225, 100548395, 100535977, 100439614.\n",
    "\n",
    "Grupo de prácticas: D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71344599-b9b7-448b-94bd-f132c660b9cc",
   "metadata": {},
   "source": [
    "## Índice\n",
    "- [1. Introducción](#1.-Introducción)\n",
    "\n",
    "- [2. Lectura de los datos](#2.-Lectura-de-los-datos)\n",
    "\n",
    "    - [2.1 Trip data](#2.1-Trip-data)\n",
    " \n",
    "    - [2.2 Zone data](#2.2-Zone-data)\n",
    "\n",
    "- [3. Análisis exploratorio de los datos](#2.-Análisis-exploratorio-de-los-datos)\n",
    "      \n",
    "    - [3.1 Trip data](#3.1-Trip-data)\n",
    " \n",
    "    - [3.2 Zone data](#3.2-Zone-data)\n",
    "\n",
    "- [4. Preproceso de los datos](#4.-Preproceso-de-los-datos)\n",
    "\n",
    "    - [4.1 Conversión de formato de fecha al formato UNIX (timestamp)](#4.1-Conversión-de-formato-de-fecha-al-formato-UNIX-(timestamp))\n",
    "      \n",
    "    - [4.2 Unión de las 2 fuentes de datos](#4.2-Unión-de-las-fuentes-de-datos)\n",
    " \n",
    "    - [4.3 Conversión de millas a kilómetros](#4.2-Conversión-de-millas-a-kilómetros)\n",
    "\n",
    "- [5. Implementación de las consultas](#5.-Implementación-de-las-consultas)\n",
    "\n",
    "    - [5.1 Velocidad media de los taxis en función de la hora](#5.1-Velocidad-media-de-los-taxis-en-función-de-la-hora)\n",
    " \n",
    "        - [5.1.1 Dataframes de Spark](#5.1.1-Dataframes-de-Spark)\n",
    "          \n",
    "        - [5.1.2 SparkSQL](#5.1.2-SparkSQL)\n",
    "     \n",
    "        - [5.1.3 RDDs](#5.1.3-RDDs)\n",
    "      \n",
    "    - [5.2 Viajes en taxi más comunes](#5.2-Viajes-en-taxi-más-comunes)\n",
    " \n",
    "        - [5.2.1 Dataframes de Spark](#5.2.1-Dataframes-de-Spark)\n",
    "          \n",
    "        - [5.2.2 SparkSQL](#5.2.2-SparkSQL)\n",
    "     \n",
    "        - [5.2.3 RDDs](#5.2.3-RDDs)\n",
    "     \n",
    "    - [5.3 Cantidad total media pagada en función del número de pasajeros](#5.2-Cantidad-total-media-pagada-en-función-del-número-de-pasajeros)\n",
    " \n",
    "        - [5.3.1 Dataframes de Spark](#5.3.1-Dataframes-de-Spark)\n",
    "          \n",
    "        - [5.3.2 SparkSQL](#5.3.2-SparkSQL)\n",
    "     \n",
    "        - [5.3.3 RDDs](#5.3.3-RDDs)\n",
    "\n",
    "- [6. Análisis del rendimiento comparando las distintas consultas](#6.-Análisis-del-rendimiento-comparando-las-distintas-consultas)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "\n",
    "  \n",
    "- [Conclusión](#conclusión)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4cd89-a773-478c-b22e-3ccbde6c6203",
   "metadata": {},
   "source": [
    "## 1. Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39943876-4c4e-448d-a03c-59cb46fa2044",
   "metadata": {},
   "source": [
    "Este proyecto consiste en un análisis de los datos de viajes de taxis de YellowCab en la ciudad de Nueva York, utilizando Apache Spark para procesar grandes volúmenes de datos. Se realizan estudios sobre:\n",
    "\n",
    "- Velocidad media de los taxis en función de la hora.\n",
    "  \n",
    "- Viajes en taxi más comunes.\n",
    "\n",
    "- Cantidad total media pagada en función del número de pasajeros.\n",
    "  \n",
    "Esto se consigue mediante la implementación de consultas mediante Spark SQL, DataFrames y RDDs. El objetivo principal es realizar un análisis de rendimiento comparativo entre estas técnicas.\n",
    "\n",
    "\n",
    "`revisar introducción`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6553faa-1bcb-4f63-9b4d-90834d8d91a7",
   "metadata": {},
   "source": [
    "## 2. Lectura de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d587585b-80a0-4318-923f-7b761e0b8b5c",
   "metadata": {},
   "source": [
    "### 2.1 Trip data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd901179-b6ad-401d-b895-d382fec22def",
   "metadata": {},
   "source": [
    "En primer lugar, leemos el conjunto de datos principal, que se encontrará almacenado en el archivo `data/tripdata_2017-01.csv`. Este fichero contiene datos estructurados sobre los viajes en taxi en la ciudad de Nueva York durante el mes de Enero de 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff45682f-2490-41f7-8c83-89768136fbff",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "cores = 3\n",
    "spark = SparkSession.builder.master(f'local[{cores}]').getOrCreate()\n",
    "#spark.conf.set('spark.cores.max', '1')\n",
    "#spark.conf.set('spark.driver.memory','1000M')\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe02c97-1321-427c-b356-81226826307d",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Definimos la ruta del archivo CSV\n",
    "route_trip_data = \"data/tripdata_2017-01.csv\"\n",
    "\n",
    "# Leemos el archivo CSV como un DataFrame de Spark\n",
    "trip_data_df = spark.read.csv(route_trip_data, header=True, inferSchema=True)\n",
    "\n",
    "print(type(trip_data_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e2304-de61-4fd6-99e5-faf0e993cf7b",
   "metadata": {},
   "source": [
    "Se obtiene un objeto dataframe de Spark con los datos almacenados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82861f02-9719-477a-98e9-f8daf2c3c73b",
   "metadata": {},
   "source": [
    "### 2.2 Zone data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2635ee7c-7f3c-4b06-b18f-bc1eae360fd1",
   "metadata": {},
   "source": [
    "De la misma manera, leemos el archivo CSV que contiene información sobre las distintas zonas de la ciudad de Nueva York. Este archivo se incluye en la práctica con el objetivo de ampliar nuestro dataset principal, añadiendo información sobre las zonas en que se tomaron los taxis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6212e8f6-d22c-4146-8c49-cc261c1dbdde",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Leemos el fichero con información sobre las zonas\n",
    "route_zone_data = \"data/taxi_zone_lookup.csv\"\n",
    "\n",
    "# Leemos el archivo CSV como un DataFrame de Spark\n",
    "zone_data_df = spark.read.csv(route_zone_data, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8e7a7-16e7-4901-b4ce-b19520ea46d3",
   "metadata": {},
   "source": [
    "## 3. Análisis exploratorio de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc518c3d-b2c4-4b69-8787-6c413f4b9e67",
   "metadata": {},
   "source": [
    "Antes de empezar con las operaciones y el análisis, vamos a realizar un pequeño análisis exploratorio de datos (EDA). De esta manera, podremos investigar y resumir las características más importantes de nuestros datasets. Esto nos servirá para comprender mejor los conjuntos de datos e identificar potenciales operaciones de preprocesamiento que serán necesarias para nuestra práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0bc5a7-02dd-4a9a-bf70-a3abaa2efe97",
   "metadata": {},
   "source": [
    "### 3.1 Trip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f8f142-c56a-471e-adab-57dae0311884",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "trip_data_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13282e2f-a417-43c1-a640-61ab44103e95",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "trip_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dea9cc-6740-414c-834e-d749d8289eb5",
   "metadata": {},
   "source": [
    "El conjunto de datos se compone de **971010 filas y 17 atributos**. En cuanto a las columnas, 8 contienen valores de tipo double, 6 contienen valores enteros (integer), 2 contienen valores de tipo timestamp (fecha) y 1 contiene valores de tipo string. Inspeccionando la página web [NYC TLC Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page), podemos encontrar el significado de todos estos atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d6bdae0-c6fd-4534-94cf-345601cdef14",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "trip_data_df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db301765-a46d-494f-a47a-923bf20cf562",
   "metadata": {},
   "source": [
    "### 3.2 Zone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce60584b-c64f-455f-af12-df1ee3c19c8e",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "zone_data_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fa27797-ac51-452e-a1e3-52b736ffd62c",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "zone_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40747c85-27da-4149-8731-1c270727f51d",
   "metadata": {},
   "source": [
    "El conjunto de datos se compone de **265 filas y 4 atributos**. Cada fila se corresponde con una zona distinta de Nueva York, identificada por la columna `LocationID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "352fe25e-6c30-4985-93bf-4d1ae72808f8",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "zone_data_df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc8a1e-cff9-41e0-a970-bc4d59d55d4c",
   "metadata": {},
   "source": [
    "## 4. Preproceso de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00e2ea-c4aa-4574-87f9-64490425340a",
   "metadata": {},
   "source": [
    "Antes de empezar con las distintas operaciones, realizamos un preproceso de los datos para tenerlos en el formato más adecuado y deseado para la realización de la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9269c7-6724-4dc9-88a0-8568baa26e88",
   "metadata": {},
   "source": [
    "### 4.1 Conversión de formato de fecha al formato UNIX (timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05f0750-e812-4766-9f68-fe0287b1d098",
   "metadata": {},
   "source": [
    "En el apartado anterior, observamos que las columnas **tpep_pickup_datetime** y **tpep_dropoff_datetime** contienen fechas (como era de esperar) en formato cadena (string) con el formato aaaa-mm-dd hh:mm:ss. \n",
    "\n",
    "Para poder trabajar y realizar operaciones matemáticas con estos valores de fechas, es necesario convertirlas a un formato que permita cálculos y comparaciones eficientes, como el formato UNIX (timestamp), el cual representa el número de segundos transcurridos desde el 1 de enero de 1970. Añadimos 2 columnas con estos valores a nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64a2079a-56b3-4f8f-987d-66c61182e6c0",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "datetime_to_UNIX = lambda dt: int(dt.timestamp())\n",
    "datetime_to_UNIX_udf = udf(datetime_to_UNIX, IntegerType())\n",
    "\n",
    "trip_data_df_unix = (\n",
    "    trip_data_df\n",
    "    .withColumn(\"tpep_pickup_datetime_unix\", datetime_to_UNIX_udf(trip_data_df['tpep_pickup_datetime']))\n",
    "    .withColumn(\"tpep_dropoff_datetime_unix\", datetime_to_UNIX_udf(trip_data_df['tpep_dropoff_datetime']))\n",
    ")\n",
    "\n",
    "trip_data_df_unix.printSchema()\n",
    "trip_data_df_unix.select(\"tpep_pickup_datetime_unix\", \"tpep_dropoff_datetime_unix\").limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec18c1-655f-4fb5-9057-fbf946c79e31",
   "metadata": {},
   "source": [
    "### 4.2 Unión de las 2 fuentes de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc9062-5a15-4908-8226-fa687b1a4099",
   "metadata": {},
   "source": [
    "Para ampliar nuestro conjunto de datos y agregar información adicional, vamos a incorporar detalles sobre las zonas de Nueva York en las que inició y finalizó cada viaje de taxi. Para ello, realizamos dos uniones (joins) entre nuestro conjunto de datos de viajes y el conjunto de datos de zonas, utilizando el atributo LocationID del dataset de zonas y los atributos PULocationID (zona de inicio del viaje o donde se activó el taxímetro) y DOLocationID (zona de finalización del viaje o donde se apagó el taxímetro) del dataset de viajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34aa1fb2-9dab-4ac0-bb6b-9c38ac4d9ae6",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Hacemos el join por la el ID de Localización en que se inició el trayecto y seleccionamos las columnas con los nombres necesarios\n",
    "trip_data_df_unix_joined = (\n",
    "    trip_data_df_unix.alias(\"trip_data\")\n",
    "    .join(zone_data_df.alias(\"pickup_zone\"), F.col(\"trip_data.PULocationID\") == F.col(\"pickup_zone.LocationID\"), \"inner\")\n",
    "    .join(zone_data_df.alias(\"dropoff_zone\"), F.col(\"trip_data.DOLocationID\") == F.col(\"dropoff_zone.LocationID\"), \"inner\")\n",
    "    .select(\n",
    "        \"trip_data.*\",  # Todas las columnas originales de trip_data\n",
    "        F.col(\"pickup_zone.service_zone\").alias(\"Pickup_service_zone\"),  # Seleccionamos y renombramos las columnas de pickup_zone\n",
    "        F.col(\"pickup_zone.Borough\").alias(\"Pickup_Borough\"),\n",
    "        F.col(\"pickup_zone.Zone\").alias(\"Pickup_Zone\"),\n",
    "        F.col(\"dropoff_zone.service_zone\").alias(\"Dropoff_service_zone\"),  # Seleccionamos y renombramos las columnas de dropff_zone\n",
    "        F.col(\"dropoff_zone.Borough\").alias(\"Dropoff_Borough\"),\n",
    "        F.col(\"dropoff_zone.Zone\").alias(\"Dropoff_Zone\")\n",
    "    )\n",
    ")\n",
    "\n",
    "trip_data_df_unix_joined.printSchema()\n",
    "\n",
    "trip_data_df_unix_joined.select(\"Pickup_service_zone\",\n",
    "                               \"Pickup_Borough\",\n",
    "                                \"Pickup_Zone\",\n",
    "                                \"Dropoff_service_zone\",\n",
    "                                \"Dropoff_Borough\",\n",
    "                                \"Dropoff_Zone\").limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa74d5-f338-4c88-8721-2595f4109c85",
   "metadata": {},
   "source": [
    "### 4.3 Conversión de millas a kilómetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa7808-f339-4b5e-a92d-285de66efb98",
   "metadata": {},
   "source": [
    "Realizamos una conversión de millas a km ya que estamos más acostumbrados a trabajar con km y tenemos mejor la referencia de estas unidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aae8da73-bbdf-4fe5-bb62-859a52c0504f",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Convertimos y sobrescribimos la columna trip_distance en kilómetros\n",
    "trip_data_df_unix_joined = trip_data_df_unix_joined.withColumn(\"trip_distance\", F.col(\"trip_distance\") * 1.60934)\n",
    "\n",
    "# Mostramos los primeros registros para verificar la conversión\n",
    "trip_data_df_unix_joined.select(\"trip_distance\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44101b-79a0-4aba-b7ef-50078a76ed1a",
   "metadata": {},
   "source": [
    "## 5. Implementación de las consultas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c0063-83f1-4919-9b07-4d9122dc304e",
   "metadata": {},
   "source": [
    "Una vez finalizado el análisis exploratorio y el preprocesamiento del conjunto de datos, y habiendo preparado los datos en el formato adecuado para su análisis, pasamos a abordar el objetivo principal de esta práctica: implementar consultas en Spark utilizando tres enfoques diferentes (SparkSQL, DataFrames de Spark y RDDs) y analizar el rendimiento de cada método.\n",
    "\n",
    "Para lograr esto, partiremos del DataFrame `trip_data_df_unix_joined`, que contiene las fechas en formato UNIX y la información completa sobre las zonas de inicio y finalización de cada trayecto. La comparativa de rendimiento entre estos métodos nos permitirá entender las ventajas y limitaciones de cada enfoque en Spark y decidir cuál se adapta mejor según los distintos casos de uso en procesamiento de datos a gran escala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f54c81-eda7-44c2-b4d2-41028a087e6b",
   "metadata": {},
   "source": [
    "### 5.1 Velocidad media de los taxis en función de la hora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034daff-fe7c-4468-8fb1-01b680f5701c",
   "metadata": {},
   "source": [
    "A continuación, vamos a implementar las consultas para la obtención de la velocidad media de los taxis en función de la hora. \n",
    "La lógica general que seguimos para este proceso es la siguiente:\n",
    "\n",
    "1. **Calcular la duración del viaje en horas**: Usaremos los timestamps de inicio y finalización del trayecto (tpep_pickup_datetime y tpep_dropoff_datetime) para calcular la duración en horas.\n",
    "   \n",
    "2. **Calcular la velocidad media**: La velocidad media se obtiene dividiendo la distancia del viaje en km (trip_distance) entre la duración del viaje en horas.\n",
    "\n",
    "3. **Extraer la hora de recogida**: Para analizar la velocidad media por hora del día, extraemos la hora de la columna tpep_pickup_datetime.\n",
    "\n",
    "4. **Agrupar por la hora y calcular la media**.\n",
    "\n",
    "En el paso 2, debemos tener especial cuidado con los casos en los que la duración del viaje sea igual a 0, ya que esto resultaría en una división entre cero.\n",
    "\n",
    "Es importante destacar que, mientras que tanto los DataFrames como SparkSQL manejan las divisiones por cero sin generar errores (en tales casos, el resultado se devuelve como un valor nulo, que se ignora al calcular promedios), los RDDs sí que arrojan errores en estas operaciones. Por lo tanto, antes de proceder con los cálculos, es fundamental verificar si existen viajes con una duración de 0 en nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d054523d-81c9-457f-b2d9-17b7b15d14e5",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Filtrar el DataFrame donde la duración es 0\n",
    "zero_duration_df = trip_data_df_unix_joined.filter(\n",
    "    (trip_data_df_unix_joined[\"tpep_dropoff_datetime_unix\"] - trip_data_df_unix_joined[\"tpep_pickup_datetime_unix\"]) == 0\n",
    ")\n",
    "\n",
    "# Contar cuántas filas tienen duración 0\n",
    "zero_duration_count = zero_duration_df.count()\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(f\"Número de viajes con duración 0: {zero_duration_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b19c59d-1007-4247-b14c-16f162dd4849",
   "metadata": {},
   "source": [
    "Observamos que existen viajes con una duración de 0. Por lo tanto, con el objetivo de que todas las consultas realicen pasos similares y así ser más exactos en la comparación del rendimiento entre los diferentes enfoques, procederemos a filtrar las filas donde la duración del viaje sea mayor que 0 en las tres consultas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9636e-3eb8-4b3a-87fb-7ff7cab40f87",
   "metadata": {},
   "source": [
    "#### 5.1.1 Dataframes de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53d143b1-c0a8-435d-a625-7ff832850116",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# 1. Calculamos la duración del viaje en horas\n",
    "trip_data_df_unix_joined_df = trip_data_df_unix_joined.withColumn(\n",
    "    \"trip_duration_hours\",\n",
    "    (F.col(\"tpep_dropoff_datetime_unix\") - F.col(\"tpep_pickup_datetime_unix\")) / 3600\n",
    ")\n",
    "\n",
    "# 2. Filtramos las filas donde la duración del viaje es mayor que 0\n",
    "trip_data_df_unix_joined_df = trip_data_df_unix_joined_df.filter(F.col(\"trip_duration_hours\") > 0)\n",
    "\n",
    "# 3. Calculamos la velocidad media en función de la duración y distancia del viaje\n",
    "trip_data_df_unix_joined_df = trip_data_df_unix_joined_df.withColumn(\n",
    "    \"avg_speed\",\n",
    "    F.col(\"trip_distance\") / F.col(\"trip_duration_hours\")\n",
    ")\n",
    "\n",
    "# 4. Extraemos la hora de la columna tpep_pickup_datetime para hacer el análisis por hora del día\n",
    "trip_data_df_unix_joined_df = trip_data_df_unix_joined_df.withColumn(\n",
    "    \"pickup_hour\",\n",
    "    F.hour(\"tpep_pickup_datetime\")\n",
    ")\n",
    "\n",
    "# 5. Agrupamos por la hora del día y calcular la velocidad media\n",
    "avg_speed_by_hour = trip_data_df_unix_joined_df.groupBy(\"pickup_hour\").agg(\n",
    "    F.avg(\"avg_speed\").alias(\"average_speed\")\n",
    ").orderBy(\"pickup_hour\")\n",
    "\n",
    "# Medimos el tiempo que de inicio y final de la consulta\n",
    "initial_time_avg_speed_df = time.time()\n",
    "avg_speed_by_hour.show(24) # Realizamos la consulta\n",
    "final_time_avg_speed_df = time.time()\n",
    "\n",
    "elapsed_time_avg_speed_df = final_time_avg_speed_df - initial_time_avg_speed_df\n",
    "\n",
    "print(\"Tiempo de ejecución (DataFrames): \", elapsed_time_avg_speed_df, \"segundos\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148e68d-2470-4fc6-9a55-cfdea2531a58",
   "metadata": {},
   "source": [
    "#### 5.1.2 SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d0ac873-5347-475a-a5c3-246f236eb29c",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Registramos el DataFrame original como una vista temporal para poder usar SQL\n",
    "trip_data_df_unix_joined.createOrReplaceTempView(\"trip_data\")\n",
    "\n",
    "# Ejecutamos la consulta unificada en SQL\n",
    "avg_speed_by_hour_sql_query = \"\"\"\n",
    "    SELECT \n",
    "        hour(tpep_pickup_datetime) AS pickup_hour,\n",
    "        AVG(trip_distance / ((tpep_dropoff_datetime_unix - tpep_pickup_datetime_unix) / 3600)) AS average_speed\n",
    "    FROM \n",
    "        trip_data\n",
    "    WHERE \n",
    "        (tpep_dropoff_datetime_unix - tpep_pickup_datetime_unix) > 0  -- Para evitar divisiones por cero\n",
    "    GROUP BY \n",
    "        hour(tpep_pickup_datetime)\n",
    "    ORDER BY \n",
    "        pickup_hour\n",
    "\"\"\"\n",
    "\n",
    "# Medimos el tiempo de ejecución de la consulta\n",
    "initial_time_avg_speed_sql = time.time()\n",
    "spark.sql(avg_speed_by_hour_sql_query).show(24) # Realizamos la consulta\n",
    "final_time_avg_speed_sql = time.time()\n",
    "\n",
    "elapsed_time_avg_speed_sql = final_time_avg_speed_sql - initial_time_avg_speed_sql\n",
    "\n",
    "# Imprimimos el tiempo de ejecución y el resultado en formato tabla\n",
    "print(\"Tiempo de ejecución (SparkSQL): \", elapsed_time_avg_speed_sql, \"segundos\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59afbf-5896-49d0-b85e-4e01eedc7967",
   "metadata": {},
   "source": [
    "#### 5.1.3 RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77555cff-3feb-4191-a656-d9c1fa79666c",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Convertimos el DataFrame a RDD\n",
    "trip_data_rdd = trip_data_df_unix_joined.rdd\n",
    "\n",
    "# Definimos una función para calcular la velocidad media y extraer la hora de recogida\n",
    "def calculate_speed(row):\n",
    "    # Calculamos la duración del viaje en horas\n",
    "    trip_duration_hours = (row['tpep_dropoff_datetime_unix'] - row['tpep_pickup_datetime_unix']) / 3600\n",
    "    # Extraemos la hora de la columna de fecha y hora de recogida\n",
    "    pickup_hour = row['tpep_pickup_datetime'].hour\n",
    "    return (pickup_hour, trip_duration_hours, row['trip_distance'])\n",
    "\n",
    "\n",
    "# Medimos el tiempo de ejecución\n",
    "initial_time_avg_speed_rdd = time.time()\n",
    "# Realizamos la consulta\n",
    "avg_speed_by_hour_rdd = (\n",
    "    trip_data_rdd\n",
    "    .map(calculate_speed)                          # Calculamos la duración del viaje y recogemos la información necesaria\n",
    "    .filter(lambda x: x[1] > 0)                    # Filtramos filas donde la duración es mayor que 0\n",
    "    .map(lambda x: (x[0], x[2] / (x[1])))          # Calculamos la velocidad media\n",
    "    .groupByKey()                                   # Agrupamos por hora\n",
    "    .mapValues(lambda speeds: sum(speeds) / len(speeds))  # Calculamos el promedio\n",
    "    .sortByKey()                                    # Ordenamos por la hora\n",
    ")\n",
    "# Convertimos a DataFrame y ejecutamos la consulta\n",
    "avg_speed_by_hour_rdd.toDF([\"pickup_hour\", \"average_speed\"]).show(24)\n",
    "# Medimos el tiempo final\n",
    "final_time_avg_speed_rdd = time.time()\n",
    "\n",
    "elapsed_time_avg_speed_rdd = final_time_avg_speed_rdd - initial_time_avg_speed_rdd\n",
    "\n",
    "# Imprimimos el tiempo de ejecución y el resultado\n",
    "print(\"Tiempo de ejecución (RDD): \", elapsed_time_avg_speed_rdd, \"segundos\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703b158-db10-4738-ad31-ad01a078a9c2",
   "metadata": {},
   "source": [
    "### 5.2 Viajes en taxi más comunes\n",
    "Los pasos que seguimos para obtener los viajes en taxi más comunes en base a las zonas son los siguientes:\n",
    "1. **Agrupación**: Agrupamos por Pickup_Zone y Dropoff_Zone, lo que permite identificar combinaciones de origen y destino.\n",
    "   \n",
    "2. **Conteo**: Utilizamos .count() para contar la cantidad de veces que cada combinación aparece.\n",
    "\n",
    "3. **Orden**: Ordenamos el resultado en orden descendente por el número de ocurrencias.\n",
    "   \n",
    "4. **Visualización**: Se muestran los primeros 10 resultados (10 viajes más comunes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a585fb4e-cab7-4bdf-b4a0-1085724ebe24",
   "metadata": {},
   "source": [
    "#### 5.2.1 Dataframes de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bae8194a-293d-4320-a8b1-680c407910cf",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Agrupamos por las columnas de ubicación de recogida y destino y contamos las ocurrencias\n",
    "most_common_trips_df = (trip_data_df_unix_joined\n",
    "    .groupBy(\"Pickup_Zone\", \"Dropoff_Zone\")\n",
    "    .count()  # Contamos el número de viajes por combinación de PULocationID y DOLocationID\n",
    "    .orderBy(F.desc(\"count\"))  # Ordenamos en orden descendente para ver las más comunes primero\n",
    ")\n",
    "\n",
    "# Medimos el tiempo de ejecución\n",
    "initial_time_common_df = time.time()\n",
    "# Mostramos los 10 viajes más comunes\n",
    "most_common_trips_df.show(10)\n",
    "# Medimos el tiempo final\n",
    "final_time_common_df = time.time()\n",
    "\n",
    "elapsed_time_common_df = final_time_common_df - initial_time_common_df\n",
    "\n",
    "# Imprimimos el tiempo de ejecución y el resultado\n",
    "print(\"Tiempo de ejecución (DataFrames): \", elapsed_time_common_df, \"segundos\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80da7f2-d96e-4a3a-b21a-8558e31da64b",
   "metadata": {},
   "source": [
    "#### 5.2.2 SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c849dd4-ad9e-48f8-b2d9-9197738cd4d0",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Registramos el DataFrame como una vista temporal\n",
    "trip_data_df_unix_joined.createOrReplaceTempView(\"trip_data_common_trips\")\n",
    "\n",
    "# Consulta SQL para obtener los 10 viajes más comunes entre zonas de recogida y destino\n",
    "most_common_trips_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Pickup_Zone, \n",
    "        Dropoff_Zone, \n",
    "        COUNT(*) AS trip_count\n",
    "    FROM \n",
    "        trip_data_common_trips\n",
    "    GROUP BY \n",
    "        Pickup_Zone, Dropoff_Zone\n",
    "    ORDER BY \n",
    "        trip_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "# Medimos el tiempo de ejecución\n",
    "initial_time_common_sql = time.time()\n",
    "# Mostramos los resultados\n",
    "most_common_trips_sql.show(10)\n",
    "# Medimos el tiempo final\n",
    "final_time_common_sql = time.time()\n",
    "\n",
    "elapsed_time_common_sql = final_time_common_sql - initial_time_common_sql\n",
    "\n",
    "# Imprimimos el tiempo de ejecución y el resultado\n",
    "print(\"Tiempo de ejecución (SparkSQL): \", elapsed_time_common_sql, \"segundos\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81cb886-4dff-4ad7-8481-497eb9d09924",
   "metadata": {},
   "source": [
    "#### 5.2.3 RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e76206cd-5f1b-4542-b882-8dceda99bc91",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Convertimos el DataFrame a RDD\n",
    "trip_data_rdd = trip_data_df_unix_joined.select(\"Pickup_Zone\", \"Dropoff_Zone\").rdd\n",
    "\n",
    "# Medimos el tiempo inicial\n",
    "initial_time_common_rdd = time.time()\n",
    "\n",
    "# Calculamos los viajes más comunes\n",
    "most_common_trips_rdd = (trip_data_rdd\n",
    "    .map(lambda row: ((row[\"Pickup_Zone\"], row[\"Dropoff_Zone\"]), 1))  # Creamos pares clave-valor con (Pickup_Zone, Dropoff_Zone) y cuenta 1\n",
    "    .reduceByKey(lambda a, b: a + b)  # Sumamos las cuentas para cada combinación de zonas\n",
    "    .sortBy(lambda x: x[1], ascending=False)  # Ordenamos en orden descendente por la cuenta\n",
    ")\n",
    "most_common_trips_rdd.toDF([\"Zones\", \"Count\"]).show(10)\n",
    "# Medimos el tiempo final\n",
    "final_time_common_rdd = time.time()\n",
    "\n",
    "elapsed_time_common_rdd = final_time_common_rdd - initial_time_common_rdd\n",
    "\n",
    "# Imprimimos el tiempode ejecución y el resultado\n",
    "print(\"Tiempo de ejecución (RDD): \", elapsed_time_common_rdd, \"segundos\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d2167-d8de-4c70-80c6-7ff627a95b28",
   "metadata": {},
   "source": [
    "La columna Zones se trata de una tupla indicando el origen y el destino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d81d7f8c-1774-4b24-9a89-8615cb4ff7dc",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Tomamos los 10 viajes más comunes\n",
    "top_10_common_trips = most_common_trips_rdd.take(10)\n",
    "\n",
    "# Convertimos el resultado a un DataFrame de Pandas para mostrarlo\n",
    "top_10_df = spark.createDataFrame(top_10_common_trips, [\"Zones\", \"Count\"]).toPandas()\n",
    "\n",
    "top_10_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305a647-c359-4deb-bdc0-6aff7ed15a36",
   "metadata": {},
   "source": [
    "### 5.3 Cantidad total media pagada en función del número de pasajeros\n",
    "Los pasos seguidos para realizar esta consulta son los siguientes:\n",
    "\n",
    "1. **Agrupar por número de pasajeros**\n",
    "\n",
    "2. **Calcular la media** de la cantidad total pagada para cada número de pasajeros\n",
    "\n",
    "3. **Ordenar por número de pasajeros**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c48f1-39dc-44df-9f27-fc95b78ec50a",
   "metadata": {},
   "source": [
    "#### 5.3.1 Dataframes de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "240831d7-b5e3-49f3-9e8a-051de3dd1046",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Medir el tiempo de ejecución\n",
    "start_time_total_df = time.time()\n",
    "# Calculamos la cantidad total media pagada en función del número de pasajeros\n",
    "avg_total_paid_by_passengers_df = (\n",
    "    trip_data_df_unix_joined\n",
    "    .groupBy(\"passenger_count\")\n",
    "    .agg(F.avg(\"total_amount\").alias(\"average_total_paid\"))\n",
    "    .orderBy(\"passenger_count\")\n",
    ")\n",
    "# Mostramos los resultados\n",
    "avg_total_paid_by_passengers_df.show()\n",
    "# Medimos el tiempo final\n",
    "end_time_total_df = time.time()\n",
    "\n",
    "elapsed_time_total_df = end_time_total_df - start_time_total_df\n",
    "\n",
    "print(\"Tiempo de ejecución (DataFrames):\", elapsed_time_total_df, \"segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d13da-105f-4eac-b711-1820bfcc2981",
   "metadata": {},
   "source": [
    "#### 5.3.2 SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bdee8e3-1a7b-400c-881e-db1e70edaa23",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Registrar el DataFrame como una vista temporal\n",
    "trip_data_df_unix_joined.createOrReplaceTempView(\"trip_data\")\n",
    "\n",
    "# Ejecutar la consulta SQL para calcular la cantidad total media pagada\n",
    "avg_total_paid_by_passengers_sql = spark.sql(\"\"\"\n",
    "    SELECT passenger_count, AVG(total_amount) AS average_total_paid\n",
    "    FROM trip_data\n",
    "    GROUP BY passenger_count\n",
    "    ORDER BY passenger_count\n",
    "\"\"\")\n",
    "\n",
    "# Medir el tiempo de ejecución\n",
    "start_time_total_sql = time.time()\n",
    "# Mostrar los resultados\n",
    "avg_total_paid_by_passengers_sql.show()\n",
    "# Medir el tiempo final\n",
    "end_time_total_sql = time.time()\n",
    "\n",
    "elapsed_time_total_sql = end_time_total_sql - start_time_total_sql\n",
    "\n",
    "print(\"Tiempo de ejecución (Spark SQL):\", elapsed_time_total_sql, \"segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a311d4-ade2-427e-a839-c631c0355498",
   "metadata": {},
   "source": [
    "#### 5.3.3 RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e54b146e-3613-4e7c-9c42-44c5943317bb",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Convertimos el DataFrame a RDD\n",
    "trip_data_total_rdd = trip_data_df_unix_joined.rdd\n",
    "\n",
    "# Medimos el tiempo de ejecución\n",
    "start_time_total_rdd = time.time()\n",
    "# Calculamos la cantidad total media pagada en función del número de pasajeros\n",
    "avg_total_paid_by_passengers_rdd = (\n",
    "    trip_data_total_rdd\n",
    "    .map(lambda row: (row[\"passenger_count\"], (row[\"total_amount\"], 1)))  # Creamos pares clave-valor\n",
    "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))  # Sumamos total_amount y contamos el número de ocurrencias para un mismo número de pasajeros\n",
    "    .mapValues(lambda x: x[0] / x[1])  # Calculamos la media\n",
    "    .sortByKey()  # Ordenamos por número de pasajeros\n",
    ")\n",
    "# Convertimos a DataFrame para obtener los datos y visualizarlos\n",
    "avg_total_paid_by_passengers_rdd.toDF([\"passenger_count\", \"average_total_paid\"]).show()\n",
    "# Medimos el tiempo final\n",
    "end_time_total_rdd = time.time()\n",
    "\n",
    "elapsed_time_total_rdd = end_time_total_rdd - start_time_total_rdd\n",
    "\n",
    "print(\"Tiempo de ejecución (RDDs):\", elapsed_time_total_rdd, \"segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340531e-d73b-4ee5-a2d9-8a1672c604f5",
   "metadata": {},
   "source": [
    "## 6. Análisis del rendimiento comparando las distintas consultas\n",
    "En esta sección, se llevará a cabo un análisis detallado del rendimiento de las consultas ejecutadas, comparando los tres enfoques diferentes: **DataFrames, SQL y RDDs**. Se evaluará la eficiencia de cada método en términos de tiempo de ejecución y speedup, permitiendo identificar cuál de estos enfoques es más adecuado para realizar consultas sobre grandes volúmenes de datos en Apache Spark. Se presentarán los resultados obtenidos, así como gráficos que faciliten la interpretación de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb20b430-d333-4d69-8290-03bd8929c6a1",
   "metadata": {},
   "source": [
    "### 6.1 Análisis de tiempos de ejecución de las consultas\n",
    "Para facilitar la comparación de los tiempos de ejecución de las consultas, se presenta una gráfica de barras que permiten visualizar de manera clara y concisa los resultados obtenidos a través de los tres enfoques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec7c0b04-fe1f-449b-a06b-12a9a67a4acc",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Datos de tiempos de ejecución\n",
    "labels = ['Velocidad Media', 'Viajes Comunes', 'Cantidad Total Media Pagada']\n",
    "df_times = [elapsed_time_avg_speed_df, elapsed_time_common_df, elapsed_time_total_df]\n",
    "sql_times = [elapsed_time_avg_speed_sql, elapsed_time_common_sql, elapsed_time_total_sql]\n",
    "rdd_times = [elapsed_time_avg_speed_rdd, elapsed_time_common_rdd, elapsed_time_total_rdd]\n",
    "\n",
    "# Configuración de la gráfica\n",
    "x = np.arange(len(labels))  # la ubicación de las etiquetas\n",
    "width = 0.25  # el ancho de las barras\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width, df_times, width, label='DataFrame', color='b')\n",
    "bars2 = ax.bar(x, sql_times, width, label='SQL', color='g')\n",
    "bars3 = ax.bar(x + width, rdd_times, width, label='RDD', color='r')\n",
    "\n",
    "# Añadimos etiquetas, título y personalizamos la gráfica\n",
    "ax.set_ylabel('Tiempo de Ejecución (segundos)')\n",
    "ax.set_title('Comparación de Tiempos de Ejecución')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Etiquetas en las barras\n",
    "for bars in (bars1, bars2, bars3):\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69713237-7c37-467d-ae6d-d5bedeb3bba3",
   "metadata": {},
   "source": [
    "#### Análisis de Resultados del Gráfico de Comparación de Tiempos de Ejecución\n",
    "\n",
    "- Al observar esta gráfica, es evidente la diferencia significativa en los tiempos de consulta entre RDDs y los enfoques de DataFrames y Spark SQL. Esta disparidad en el rendimiento se debe principalmente a que tanto Spark SQL como los DataFrames aprovechan el optimizador de consultas Catalyst, que aplica una variedad de optimizaciones a las consultas. Estas optimizaciones incluyen el reordenamiento de operaciones, la fusión de etapas y la eliminación de cálculos redundantes, lo que permite una ejecución más eficiente de las consultas. En cambio, los RDDs en Apache Spark utilizan un enfoque basado en grafos para optimizar las transformaciones, pero sus optimizaciones son más limitadas y menos específicas en comparación con las aplicadas a los DataFrames y Spark SQL.\n",
    "\n",
    "- La consulta de **\"Cantidad Total Media Pagada\"** muestra una diferencia particularmente alta en el tiempo de ejecución de los RDDs frente a los otros dos métodos. Esto se debe probablemente a la naturaleza de la consulta, que involucra cálculos complejos de agregación que Catalyst optimiza eficientemente en SQL y DataFrames. Al realizar esta consulta con RDDs, usamos `reduceByKey` para sumar el monto total pagado y contar las ocurrencias para cada número de pasajeros, mediante una operación como `reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))`. Esta operación, aunque efectiva, es más costosa en términos de tiempo de ejecución porque Spark no puede aplicar las mismas optimizaciones de Catalyst, lo que se traduce en un tiempo de procesamiento significativamente mayor.\n",
    "\n",
    "- En general, los enfoques de **DataFrames y Spark SQL** son altamente recomendables para consultas de análisis en Spark, ya que el optimizador Catalyst reduce los tiempos de ejecución de manera significativa en comparación con los RDDs. Mientras que los RDDs pueden ser útiles para transformaciones personalizadas que requieren un control granular, resultan menos eficientes para cálculos complejos de agregación y deberían usarse principalmente en casos donde Catalyst no pueda aplicar optimizaciones adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629326f7-0999-450c-839d-fcea15c8a258",
   "metadata": {},
   "source": [
    "### 6.2 Análisis del speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7a41792-f8e8-43d0-8305-80c33143e952",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Datos de tiempo de ejecución\n",
    "data = {\n",
    "    \"Consulta\": [\n",
    "        \"Velocidad Media\",\n",
    "        \"Viajes en Taxi Más Comunes\",\n",
    "        \"Cantidad Total Media Pagada\"\n",
    "    ],\n",
    "    \"DataFrame (segundos)\": [\n",
    "        elapsed_time_avg_speed_df,\n",
    "        elapsed_time_common_df,\n",
    "        elapsed_time_total_df\n",
    "    ],\n",
    "    \"SQL (segundos)\": [\n",
    "        elapsed_time_avg_speed_sql,\n",
    "        elapsed_time_common_sql,\n",
    "        elapsed_time_total_sql\n",
    "    ],\n",
    "    \"RDD (segundos)\": [\n",
    "        elapsed_time_avg_speed_rdd,\n",
    "        elapsed_time_common_rdd,\n",
    "        elapsed_time_total_rdd\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Crear un DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calcular el speedup\n",
    "df[\"Speedup (SQL vs DataFrame)\"] = df[\"SQL (segundos)\"] / df[\"DataFrame (segundos)\"]\n",
    "df[\"Speedup (RDD vs DataFrame)\"] = df[\"RDD (segundos)\"] / df[\"DataFrame (segundos)\"]\n",
    "df[\"Speedup (RDD vs SQL)\"] = df[\"RDD (segundos)\"] / df[\"SQL (segundos)\"]\n",
    "\n",
    "# Graficar el speedup\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Configuración de los gráficos\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(df))\n",
    "\n",
    "# Barras para el speedup\n",
    "bars1 = plt.bar(index, df[\"Speedup (SQL vs DataFrame)\"], bar_width, label=\"SQL vs DataFrame\", color='b')\n",
    "bars2 = plt.bar(index + bar_width, df[\"Speedup (RDD vs DataFrame)\"], bar_width, label=\"RDD vs DataFrame\", color='g')\n",
    "bars3 = plt.bar(index + 2 * bar_width, df[\"Speedup (RDD vs SQL)\"], bar_width, label=\"RDD vs SQL\", color='r')\n",
    "\n",
    "# Agregar los valores en cada barra\n",
    "for bar in bars1 + bars2 + bars3:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "\n",
    "# Personalizar el gráfico\n",
    "plt.xlabel('Consultas')\n",
    "plt.ylabel('Speedup')\n",
    "plt.title('Análisis de Speedup para Consultas en PySpark')\n",
    "plt.xticks(index + bar_width, df[\"Consulta\"])\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed54ebd-6bfa-4907-9e3f-d7b44cd75106",
   "metadata": {},
   "source": [
    "`PÁRRAFO EXPLICANDO EL SPEEDUP`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab989c-1919-40cb-a8f2-7a35392beb38",
   "metadata": {},
   "source": [
    "## 7. Análisis del rendimiento en base al número de cores utilizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead0aa38-e033-4028-b085-fa6dee0dfc9b",
   "metadata": {},
   "source": [
    "En esta sección, se lleva a cabo un análisis exhaustivo del impacto que tiene la variación en el número de cores utilizados en el rendimiento de las consultas ejecutadas con Apache Spark. Para ello, modificamos el parámetro `local[]` en la configuración de la `SparkSession`, lo que nos permite asignar diferentes cantidades de cores al procesamiento. Dado que el kernel debe ser reiniciado cada vez que se realiza un cambio en esta configuración, registramos los resultados del tiempo de ejecución en archivos externos para asegurar que la información se conserve y se pueda analizar posteriormente. Además, se analizará el speedup obtenido en función del número de cores utilizados, proporcionando una visión clara sobre cómo la paralelización afecta a la eficiencia y rendimiento de nuestras operaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a940ad-b7af-440e-8c21-545c31248867",
   "metadata": {},
   "source": [
    "Guardamos los tiempos para el número de cores en cuestión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "defd2392-8646-4c47-8dba-4fa044115a75",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Datos de tiempo de ejecución\n",
    "data = {\n",
    "    \"Consulta\": [\n",
    "        \"Velocidad Media\",\n",
    "        \"Viajes en Taxi Más Comunes\",\n",
    "        \"Cantidad Total Media Pagada\"\n",
    "    ],\n",
    "    \"DataFrame (segundos)\": [\n",
    "        elapsed_time_avg_speed_df,\n",
    "        elapsed_time_common_df,\n",
    "        elapsed_time_total_df\n",
    "    ],\n",
    "    \"SQL (segundos)\": [\n",
    "        elapsed_time_avg_speed_sql,\n",
    "        elapsed_time_common_sql,\n",
    "        elapsed_time_total_sql\n",
    "    ],\n",
    "    \"RDD (segundos)\": [\n",
    "        elapsed_time_avg_speed_rdd,\n",
    "        elapsed_time_common_rdd,\n",
    "        elapsed_time_total_rdd\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Crear un DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Guardar el DataFrame en un archivo JSON\n",
    "df.to_json(f'exe_times/tiempos_ejecucion_{cores}_cores.json')\n",
    "\n",
    "print(f\"Los datos han sido guardados en tiempos_ejecucion_{cores}_cores.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6ba09-f346-4a78-be8e-13ccf47ec9df",
   "metadata": {},
   "source": [
    "`EJECUTAR TODO MODIFICANDO LA VARIABLE cores (arriba del todo) Y ANALIZAR LA VARIACIÓN DE LOS TIEMPOS Y EL SPEEDUP!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cd7daba-bffc-4db6-87cb-e49d6aec7085",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Almacena los datos en una lista de diccionarios\n",
    "data = []\n",
    "\n",
    "# Cargar cada archivo JSON de 1 a 32 núcleos\n",
    "for cores in range(1, 32):\n",
    "    with open(f'exe_times/tiempos_ejecucion_{cores}_cores.json') as file:\n",
    "        json_data = json.load(file)\n",
    "        for method in [\"DataFrame (segundos)\", \"SQL (segundos)\", \"RDD (segundos)\"]:\n",
    "            for i, consulta in enumerate(json_data[\"Consulta\"].values()):\n",
    "                data.append({\n",
    "                    \"Núcleos\": cores,\n",
    "                    \"Consulta\": consulta,\n",
    "                    \"Método\": method.replace(\" (segundos)\", \"\"),\n",
    "                    \"Tiempo (segundos)\": json_data[method][str(i)]\n",
    "                })\n",
    "\n",
    "# Convertir a un DataFrame\n",
    "df_consultas = pd.DataFrame(data)\n",
    "print(df_consultas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76bb8b20-6846-41cd-9ecc-49ab6c494a8d",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Configurar el estilo de la gráfica\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Crear un gráfico de líneas para cada consulta\n",
    "for consulta in df_consultas[\"Consulta\"].unique():\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    subset = df_consultas[df_consultas[\"Consulta\"] == consulta]\n",
    "    sns.lineplot(data=subset, x=\"Núcleos\", y=\"Tiempo (segundos)\", hue=\"Método\", marker=\"o\")\n",
    "    \n",
    "    plt.title(f\"Tiempos de Ejecución para '{consulta}' con Diferentes Núcleos\")\n",
    "    plt.xlabel(\"Número de Núcleos\")\n",
    "    plt.ylabel(\"Tiempo de Ejecución (segundos)\")\n",
    "    plt.legend(title=\"Método\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118af21-5cde-4676-b7d2-b0d87dcba94c",
   "metadata": {},
   "source": [
    "### 1. Velocidad Media\n",
    "Al analizar los tiempos de ejecución para la consulta \"Velocidad Media\" en Apache Spark con los enfoques DataFrame, SQL y RDD, se identifican las siguientes conclusiones clave:\n",
    "\n",
    "1. **Desempeño de Cada Método**:\n",
    "   - **DataFrame y SQL**: Ambos métodos ofrecen tiempos de ejecución similares y generalmente más bajos que RDD a lo largo de la variación de núcleos, indicando que aprovechan el optimizador Catalyst en Spark. Esto mejora su rendimiento, especialmente en consultas simples.\n",
    "   - **RDD**: Muestra tiempos significativamente más altos y una mayor variabilidad con el aumento de núcleos, reflejando su falta de optimización en comparación con DataFrame y SQL.\n",
    "\n",
    "2. **Efecto de la Paralelización**:\n",
    "   - **DataFrame y SQL**: Observan una leve mejora en rendimiento al incrementar los núcleos hasta un punto de saturación (alrededor de 8-10 núcleos), tras el cual añadir más núcleos no aporta beneficios significativos debido al overhead de paralelización.\n",
    "   - **RDD**: No sigue una tendencia clara de mejora con más núcleos; en varios puntos (por ejemplo, con 25 núcleos), el tiempo de ejecución aumenta debido al overhead en la gestión de tareas, haciendo que RDD sea menos eficiente para consultas paralelas.\n",
    "\n",
    "3. **Conclusión sobre Escalabilidad**:\n",
    "   - **DataFrame y SQL** son los métodos más eficientes y adecuados para esta consulta, logrando un rendimiento estable con un número moderado de núcleos y evitando overhead adicional.\n",
    "   - **RDD** no es recomendable para este tipo de análisis en términos de rendimiento y escalabilidad, debido a su mayor variabilidad en tiempos de ejecución y falta de optimización.\n",
    "\n",
    "En resumen, para la consulta \"Velocidad Media\", **DataFrame y SQL son preferibles en Apache Spark** debido a su rendimiento superior y mejor aprovechamiento de los recursos de paralelización en comparación con RDD.\n",
    "\n",
    "\n",
    "### 2. Viajes en Taxi Más Comunes\n",
    "En la gráfica de tiempos de ejecución para la consulta \"Viajes en Taxi Más Comunes\" se identifican las siguientes observaciones:\n",
    "\n",
    "1. **Desempeño de los Métodos**:\n",
    "   - **DataFrame y SQL**: Ambos enfoques mantienen tiempos bajos y estables, con una ligera ventaja de SQL sobre DataFrame. Esto indica que ambos métodos aprovechan las optimizaciones de Catalyst en Spark, mejorando su eficiencia.\n",
    "   - **RDD**: Muestra tiempos significativamente más altos y variables, lo que evidencia su falta de optimización en comparación con DataFrame y SQL.\n",
    "\n",
    "2. **Paralelización y Número de Núcleos**:\n",
    "   - **DataFrame y SQL**: Muestran una mejora marginal con el aumento de núcleos, alcanzando un punto de saturación alrededor de 8 núcleos, donde añadir más núcleos no mejora el rendimiento.\n",
    "   - **RDD**: Su variabilidad en tiempos se incrementa especialmente a partir de 20 núcleos, lo que sugiere que el overhead de paralelización afecta negativamente su rendimiento en esta consulta.\n",
    "\n",
    "3. **Conclusiones**:\n",
    "   - **DataFrame y SQL** son los enfoques más eficientes y consistentes, alcanzando un rendimiento óptimo con alrededor de 8 núcleos.\n",
    "   - **RDD** no es adecuado para esta operación debido a su falta de eficiencia y alta variabilidad, especialmente con un mayor número de núcleos.\n",
    "\n",
    "En resumen, para la consulta \"Viajes en Taxi Más Comunes,\" **DataFrame y SQL** son las mejores opciones, destacando en estabilidad y mejor aprovechamiento de la paralelización frente a RDD.\n",
    "\n",
    "\n",
    "### 3. Cantidad Total Media Pagada\n",
    "En la gráfica de tiempos de ejecución para la consulta \"Cantidad Total Media Pagada\" se identifican las siguientes conclusiones:\n",
    "\n",
    "1. **Comparación de Métodos**:\n",
    "   - **DataFrame y SQL**: Ambos enfoques presentan tiempos de ejecución bajos y estables independientemente del número de núcleos, siendo SQL ligeramente más rápido. Esto muestra que Catalyst optimiza eficientemente estas operaciones, manteniendo un rendimiento constante.\n",
    "   - **RDD**: Exhibe tiempos de ejecución considerablemente más altos y variables. A medida que aumentan los núcleos, el tiempo de ejecución de RDD incrementa notablemente, indicando que el overhead de paralelización afecta su rendimiento negativamente.\n",
    "\n",
    "2. **Impacto de la Paralelización**:\n",
    "   - **DataFrame y SQL**: La estabilidad en sus tiempos indica que ambos métodos alcanzan un rendimiento óptimo con un número bajo de núcleos, sin necesidad de aumentar la paralelización, evitando así overhead innecesario.\n",
    "   - **RDD**: No escala bien con el incremento de núcleos, ya que el tiempo de ejecución tiende a aumentar debido al overhead de coordinación y sincronización en el procesamiento distribuido.\n",
    "\n",
    "3. **Conclusiones**:\n",
    "   - **DataFrame y SQL** son los métodos más eficientes y consistentes para esta consulta, proporcionando tiempos rápidos sin overhead adicional.\n",
    "   - **RDD** no es adecuado para esta consulta debido a su pobre escalabilidad y eficiencia.\n",
    "\n",
    "En conclusión, **DataFrame y SQL** son las opciones recomendadas para esta operación, mientras que **RDD** debería evitarse por su baja eficiencia y escalabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "677a7d7a-7b55-4a33-b4b8-c71fad7cf358",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "# Filtrar los tiempos de ejecución con 1 núcleo como referencia para cada consulta y método\n",
    "reference_times = df_consultas[df_consultas['Núcleos'] == 1].set_index(['Consulta', 'Método'])['Tiempo (segundos)']\n",
    "\n",
    "# Crear una columna de Speedup en `df_consultas` dividiendo el tiempo de referencia por el tiempo para cada número de núcleos\n",
    "df_consultas['Speedup'] = df_consultas.apply(\n",
    "    lambda row: reference_times[row['Consulta'], row['Método']] / row['Tiempo (segundos)'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Visualizar el Speedup en una gráfica para cada consulta y método\n",
    "for consulta in df_consultas['Consulta'].unique():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for metodo in df_consultas['Método'].unique():\n",
    "        subset = df_consultas[(df_consultas['Consulta'] == consulta) & (df_consultas['Método'] == metodo)]\n",
    "        plt.plot(subset['Núcleos'], subset['Speedup'], marker='o', label=metodo)\n",
    "\n",
    "    plt.xlabel(\"Número de Núcleos\")\n",
    "    plt.ylabel(\"Speedup\")\n",
    "    plt.title(f\"Speedup vs Número de Núcleos para la Consulta '{consulta}'\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3370e22-71c6-4f20-a214-8da9c5765d84",
   "metadata": {},
   "source": [
    "### 1. Análisis del Speedup para la Consulta \"Velocidad Media\"\n",
    "\n",
    "La gráfica de speedup para la consulta \"Velocidad Media\" presenta los siguientes puntos clave:\n",
    "\n",
    "1. **Aumento Inicial de Speedup**:\n",
    "   - **DataFrame y SQL**: Ambos métodos muestran un aumento de speedup hasta los 8-10 núcleos, aprovechando eficientemente la paralelización inicial.\n",
    "   - **RDD**: Logra su pico de speedup más temprano, pero con un aumento menos pronunciado, reflejando limitaciones en su escalabilidad.\n",
    "\n",
    "2. **Punto de Saturación**:\n",
    "   - DataFrame y SQL alcanzan un rendimiento óptimo alrededor de los 10 núcleos; a partir de aquí, el overhead reduce gradualmente el speedup.\n",
    "   - **RDD**: Su speedup disminuye rápidamente después de los 10 núcleos, evidenciando problemas para manejar un mayor número de núcleos.\n",
    "\n",
    "3. **Inestabilidad en Núcleos Altos**:\n",
    "   - Con más de 20 núcleos, todos los métodos experimentan una caída en speedup, especialmente **RDD**, donde el overhead de gestión supera los beneficios de paralelización.\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "Para \"Velocidad Media,\" **DataFrame y SQL** son más eficientes que **RDD**, alcanzando su mejor rendimiento con 8-10 núcleos. **RDD** presenta una escalabilidad limitada y es menos adecuado para esta consulta en configuraciones de múltiples núcleos.\n",
    "\n",
    "### 2. Análisis del Speedup para la Consulta \"Viajes en Taxi Más Comunes\"\n",
    "\n",
    "La gráfica de speedup para \"Viajes en Taxi Más Comunes\" muestra:\n",
    "\n",
    "1. **Mayor Eficiencia de SQL**:\n",
    "   - **SQL**: Alcanza un speedup superior a 3x alrededor de los 10 núcleos, indicando que aprovecha eficazmente la paralelización inicial, aunque después comienza a oscilar y reduce su eficiencia con más de 20 núcleos.\n",
    "   - **DataFrame**: Muestra un comportamiento más estable, manteniendo un speedup de aproximadamente 1.5x hasta los 15 núcleos antes de decrecer gradualmente.\n",
    "\n",
    "2. **Escalabilidad Limitada de RDD**:\n",
    "   - **RDD**: Presenta un speedup bajo y muy estable, lo que refleja su dificultad para mejorar significativamente con más núcleos debido al overhead de gestión y su falta de optimización avanzada.\n",
    "\n",
    "3. **Saturación a Partir de 20 Núcleos**:\n",
    "   - SQL y DataFrame pierden eficiencia y presentan disminuciones notables en el speedup con más de 20 núcleos, indicando que el overhead supera los beneficios de añadir núcleos adicionales.\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "Para esta consulta, **SQL es el método más eficiente en paralelización inicial**, mientras que **DataFrame** ofrece una estabilidad razonable sin tanta variabilidad. **RDD**, sin embargo, sigue mostrando limitaciones claras para beneficiarse de la paralelización.\n",
    "\n",
    "### 3. Análisis del Speedup para la Consulta \"Cantidad Total Media Pagada\"\n",
    "\n",
    "La gráfica de speedup para \"Cantidad Total Media Pagada\" revela:\n",
    "\n",
    "1. **SQL como el Método Más Eficiente**:\n",
    "   - **SQL**: Alcanza un speedup de hasta 2.5x con 8 núcleos, manteniendo un rendimiento superior a DataFrame en la mayoría de los casos. Sin embargo, su eficiencia disminuye a partir de los 20 núcleos debido al overhead de paralelización.\n",
    "   - **DataFrame**: Aunque estable, el speedup es moderado, rondando 1.5x. Se comporta de manera constante, pero sin alcanzar los picos de SQL.\n",
    "\n",
    "2. **Limitaciones de RDD**:\n",
    "   - **RDD** muestra una tendencia de speedup menor a 1.0 conforme se incrementan los núcleos, reflejando un rendimiento ineficaz y un bajo aprovechamiento de la paralelización.\n",
    "\n",
    "3. **Punto Óptimo de Paralelización**:\n",
    "   - SQL y DataFrame tienen sus mejores rendimientos con pocos núcleos (alrededor de 8-10), después de lo cual el overhead reduce su eficiencia.\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "Para esta consulta, **SQL es la mejor opción en términos de speedup inicial**, mientras que **DataFrame** es una alternativa estable. **RDD**, en cambio, sigue sin beneficiarse de la paralelización y muestra un rendimiento limitado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e62a7f-43ef-48bc-8ce2-46c090fbfade",
   "metadata": {},
   "source": [
    "## 8. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9084af74-79d8-46df-9a2c-dfbed5a558e4",
   "metadata": {},
   "source": [
    "`sacar conclusiones`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018b57cf-aa0c-4daa-9929-4ca35ab65b01",
   "metadata": {},
   "source": [
    "`REVISAR Y AÑADIR`\n",
    "### Conclusión General del Análisis de Rendimiento\n",
    "\n",
    "El análisis comparativo de rendimiento entre DataFrame, SQL y RDD en Apache Spark revela que **DataFrame y SQL** son los enfoques más eficientes y estables para tareas analíticas. Ambos métodos aprovechan el optimizador Catalyst, logrando tiempos de ejecución bajos y consistentes, incluso con variaciones en el número de núcleos. Su rendimiento alcanza un punto óptimo de paralelización con un número moderado de núcleos, lo que permite una escalabilidad adecuada sin añadir overhead innecesario.\n",
    "\n",
    "Por otro lado, **RDD** muestra mayores tiempos de ejecución y es menos eficiente, especialmente al incrementar el número de núcleos, donde el overhead de sincronización reduce su efectividad. Esto indica que RDD no es la opción ideal para tareas analíticas comunes en Spark y debería reservarse para casos específicos que requieran un control de bajo nivel en las operaciones.\n",
    "\n",
    "En conclusión, **Spark SQL y DataFrames** son las herramientas recomendadas para análisis en entornos de big data debido a su balance óptimo entre rendimiento y escalabilidad, mientras que RDD resulta menos adecuado para consultas estándar de análisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d947c3-4215-4100-9c7a-19f08fd03f94",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
